<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/youshen-apple.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/youshen-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/youshen-16x16.png">
  <link rel="mask-icon" href="/images/youshen-512x512.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.1/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"jiayuanyang.com","root":"/","images":"/images","scheme":"Muse","version":"8.0.2","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}};
  </script>
<meta name="description" content="Sentence Embedding如果需要判断句子与句子之间的相似度，仅使用上述的词向量方法显然是不行的，所以如何获得句向量也是十分重要。句向量代表了整句话的语义信息，可以帮助机器理解上下文，意图及其它重要信息。 获得句向量的方法可以归为加权平均法和模型法两大类，下面分别给出这两大类中的具体方法：">
<meta property="og:type" content="article">
<meta property="og:title" content="Sentence Embedding">
<meta property="og:url" content="https://jiayuanyang.com/2021/04/17/Sentence-Embedding/index.html">
<meta property="og:site_name" content="Migo&#39;s Blog">
<meta property="og:description" content="Sentence Embedding如果需要判断句子与句子之间的相似度，仅使用上述的词向量方法显然是不行的，所以如何获得句向量也是十分重要。句向量代表了整句话的语义信息，可以帮助机器理解上下文，意图及其它重要信息。 获得句向量的方法可以归为加权平均法和模型法两大类，下面分别给出这两大类中的具体方法：">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.loli.net/2021/04/20/XbIBMRr1kiGnFhg.png">
<meta property="og:image" content="https://i.loli.net/2021/04/20/FpgSr4TV6ID5yiv.png">
<meta property="og:image" content="https://i.loli.net/2021/04/20/7fnbVliHpwAS2u4.png">
<meta property="og:image" content="https://i.loli.net/2021/04/20/OAKUlLJrn592Ebe.png">
<meta property="og:image" content="https://i.loli.net/2021/04/20/o3Trieq8W2JuQxn.png">
<meta property="og:image" content="https://i.loli.net/2021/04/20/zvXKNA5Bb4ykqhF.png">
<meta property="og:image" content="https://i.loli.net/2021/04/20/2jvumwdr6yXZNRf.png">
<meta property="og:image" content="https://i.loli.net/2021/04/20/enbToDVaXMvFdQL.png">
<meta property="og:image" content="https://i.loli.net/2021/04/20/SMxuVcplE67aLRG.png">
<meta property="og:image" content="https://i.loli.net/2021/04/21/ODGm1YaFugpXQsV.png">
<meta property="og:image" content="https://i.loli.net/2021/04/21/wIKNWz78YQAC4uy.png">
<meta property="og:image" content="https://i.loli.net/2021/04/21/mogq1PQC5DBONds.png">
<meta property="article:published_time" content="2021-04-17T02:29:06.000Z">
<meta property="article:modified_time" content="2021-04-21T06:08:45.320Z">
<meta property="article:author" content="Migo">
<meta property="article:tag" content="embedding">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2021/04/20/XbIBMRr1kiGnFhg.png">


<link rel="canonical" href="https://jiayuanyang.com/2021/04/17/Sentence-Embedding/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>
<title>Sentence Embedding | Migo's Blog</title>
  



  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <a target="_blank" rel="noopener" href="https://github.com/YJY123" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Migo's Blog</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <section class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Sentence-Embedding"><span class="nav-number">1.</span> <span class="nav-text">Sentence Embedding</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87%E6%B3%95"><span class="nav-number">1.1.</span> <span class="nav-text">加权平均法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%B4%AF%E5%8A%A0%E6%B1%82%E5%B9%B3%E5%9D%87"><span class="nav-number">1.1.1.</span> <span class="nav-text">累加求平均</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#TF-IDF-%E5%8A%A0%E6%9D%83%E6%B1%82%E5%B9%B3%E5%9D%87"><span class="nav-number">1.1.2.</span> <span class="nav-text">TF-IDF 加权求平均</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B9%82%E5%9D%87%E5%80%BC%EF%BC%88Power-Mean%EF%BC%89%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87"><span class="nav-number">1.1.3.</span> <span class="nav-text">幂均值（Power Mean）加权平均</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SIF%EF%BC%88smooth-inverse-frequency%EF%BC%89%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87"><span class="nav-number">1.1.4.</span> <span class="nav-text">SIF（smooth inverse frequency）加权平均</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E6%B3%95"><span class="nav-number">1.2.</span> <span class="nav-text">模型法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%97%A0%E7%9B%91%E7%9D%A3%E6%A8%A1%E5%9E%8B%E6%B3%95"><span class="nav-number">1.2.1.</span> <span class="nav-text">无监督模型法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Skip-Thought-Vectors"><span class="nav-number">1.2.1.1.</span> <span class="nav-text">Skip-Thought Vectors</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Quick-Thought-Vectors"><span class="nav-number">1.2.1.2.</span> <span class="nav-text">Quick-Thought Vectors</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Doc2vec-%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.2.1.3.</span> <span class="nav-text">Doc2vec 模型</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%89%E7%9B%91%E7%9D%A3%E6%A8%A1%E5%9E%8B%E6%B3%95"><span class="nav-number">1.2.2.</span> <span class="nav-text">有监督模型法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Deep-Averaging-Networks-DAN"><span class="nav-number">1.2.2.1.</span> <span class="nav-text">Deep Averaging Networks (DAN)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Self-attentive-Sentence-Embedding"><span class="nav-number">1.2.2.2.</span> <span class="nav-text">Self-attentive Sentence Embedding</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#InferSent"><span class="nav-number">1.2.2.3.</span> <span class="nav-text">InferSent</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Sentence-BERT"><span class="nav-number">1.2.2.4.</span> <span class="nav-text">Sentence-BERT</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E6%B3%95"><span class="nav-number">1.2.3.</span> <span class="nav-text">多任务学习模型法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Universal-Sentence-Encoder"><span class="nav-number">1.2.3.1.</span> <span class="nav-text">Universal Sentence Encoder</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Reference"><span class="nav-number">2.</span> <span class="nav-text">Reference</span></a></li></ol></div>
        </section>
        <!--/noindex-->

        <section class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Migo"
      src="/images/youshen-avatar.gif">
  <p class="site-author-name" itemprop="name">Migo</p>
  <div class="site-description" itemprop="description">Migo的博客</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">7</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/YJY123" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;YJY123" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



        </section>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jiayuanyang.com/2021/04/17/Sentence-Embedding/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/youshen-avatar.gif">
      <meta itemprop="name" content="Migo">
      <meta itemprop="description" content="Migo的博客">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Migo's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Sentence Embedding
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-04-17 10:29:06" itemprop="dateCreated datePublished" datetime="2021-04-17T10:29:06+08:00">2021-04-17</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2021-04-21 14:08:45" itemprop="dateModified" datetime="2021-04-21T14:08:45+08:00">2021-04-21</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="Sentence-Embedding"><a href="#Sentence-Embedding" class="headerlink" title="Sentence Embedding"></a>Sentence Embedding</h1><p>如果需要判断句子与句子之间的相似度，仅使用上述的词向量方法显然是不行的，所以如何获得句向量也是十分重要。句向量代表了整句话的语义信息，可以帮助机器理解上下文，意图及其它重要信息。</p>
<p>获得句向量的方法可以归为<strong>加权平均法</strong>和<strong>模型法</strong>两大类，下面分别给出这两大类中的具体方法：</p>
<a id="more"></a>
<h2 id="加权平均法"><a href="#加权平均法" class="headerlink" title="加权平均法"></a>加权平均法</h2><p>加权平均法用了一种比较朴素的思想，利用句子中每个词的词向量加权来求得句向量，下面给出具体的方法：</p>
<h3 id="累加求平均"><a href="#累加求平均" class="headerlink" title="累加求平均"></a>累加求平均</h3><p>直接对句子中每个词的词向量累加后求平均作为句向量。</p>
<h3 id="TF-IDF-加权求平均"><a href="#TF-IDF-加权求平均" class="headerlink" title="TF-IDF 加权求平均"></a>TF-IDF 加权求平均</h3><p>利用TF-IDF值对句子中每个词的词向量进行加权后求平均。</p>
<h3 id="幂均值（Power-Mean）加权平均"><a href="#幂均值（Power-Mean）加权平均" class="headerlink" title="幂均值（Power Mean）加权平均"></a>幂均值（Power Mean）加权平均</h3><p>幂均值加权平均的全称为 Concatenated Power Mean Word Embeddings，即级联的幂均值词向量方法，其论文链接在参考<a href="#Reference">[1]</a>中给出，该方法的核心是<strong>幂均值</strong>和<strong>级联</strong>。</p>
<p><strong>幂均值：</strong></p>
<script type="math/tex; mode=display">(\frac{x_1^p+x_2^p+\ldots+x_n^p}{n})^{1/p}\, ; \qquad p\in \mathbb{R}\cup\{\pm \infty\}</script><p>上式中，$n$ 为句子中词的个数，$x$ 为词向量。当 $p=1$ 时，上式即为简单的加权平均法，当 $p=+∞$ 时，相当于取每个词向量各维度的最大值，当 $p=-∞$ 时，相当于取每个词向量各维度的最小值。</p>
<p><strong>级联：</strong><br>考虑到不同的预训练词向量能够学习到不同的信息，论文中使用了四种不同的词向量：GloVe, Word2Vec, Attract-Repel 和 MorphSpecialized。为了能够利用不同的信息，该方法对这四种词向量进行了拼接，如下所示：</p>
<script type="math/tex; mode=display">GV \oplus WV \oplus AR \oplus MS</script><p>论文中最后构造的词向量维度是 $4 \ast 3 \ast 300 = 3600$，其中$4$表示使用四种不同的词向量，$3$表示 $p$ 取值为 $\{-\infty, 1, +\infty\}$.</p>
<h3 id="SIF（smooth-inverse-frequency）加权平均"><a href="#SIF（smooth-inverse-frequency）加权平均" class="headerlink" title="SIF（smooth inverse frequency）加权平均"></a>SIF（smooth inverse frequency）加权平均</h3><p>SIF 【<a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=SyK00v5xx">论文</a>和<a target="_blank" rel="noopener" href="https://github.com/PrincetonML/SIF">代码</a>】是一种简单且效果很好的获取句向量的加权方法，其在语义文本相似性任务上的表现甚至超过深度学习技术，比如 InferSent等。但是，在分类任务方面，SIF稍微落后于其它深度学习模型。这可能是因为加权平均的方法并不能提供足够复杂的表示来解决诸如情感分析之类的任务。(参考<a href="#Reference">[2]</a>).</p>
<p>SIF 加权平均法的计算步骤如下所示：</p>
<ol>
<li>计算语料库中所有词的词频；</li>
<li>给定超参数 $a$，通常取值为 $1e-3\sim1e-3$，计算词向量的权重：$a/(a+p(w))$，其中 $p(w)$ 为词频；</li>
<li>使用SVD计算句向量矩阵的第一主成分 $u$，让每个句向量减去它在 $u$ （单位向量）上的矢量投影[<strong>移除所有句子的共有信息，因此保留下来的句子向量更能够表示其本身与其它句子向量之间的距离</strong>]。</li>
</ol>
<p><img src="https://i.loli.net/2021/04/20/XbIBMRr1kiGnFhg.png" alt="SIF.PNG"></p>
<h2 id="模型法"><a href="#模型法" class="headerlink" title="模型法"></a>模型法</h2><p>模型法主要是利用深度网络模型来获得句向量，可以分为无监督、有监督和多任务三大类(参考<a href="#Reference">[3]</a>)。</p>
<h3 id="无监督模型法"><a href="#无监督模型法" class="headerlink" title="无监督模型法"></a>无监督模型法</h3><h4 id="Skip-Thought-Vectors"><a href="#Skip-Thought-Vectors" class="headerlink" title="Skip-Thought Vectors"></a>Skip-Thought Vectors</h4><p>Skip-Thought Vectors【<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1506.06726">论文</a>】与 word2vec 中 skip-gram 的训练思想类似，通过一句话来预测其上一句话和下一句话。该模型是典型的 encoder-decoder 结构（GRU-RNN），只不过其有两个decoder，具体网络结构如下所示：</p>
<p><img src="https://i.loli.net/2021/04/20/FpgSr4TV6ID5yiv.png" alt="skip_thought_vector.PNG"></p>
<h4 id="Quick-Thought-Vectors"><a href="#Quick-Thought-Vectors" class="headerlink" title="Quick-Thought Vectors"></a>Quick-Thought Vectors</h4><p>Quick-Thoughts【<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.02893">论文</a>】模型是对 Skip-Thoughts 的优化，其主要优点在于训练速度的提升。Skip-Thoughts 网络结构实质上是生成模型，而且需要训练3个循环神经网络。Quick-Thoughts 则<strong>将整个过程当作分类任务</strong>来处理，具体网络结构如下所示：</p>
<p><img src="https://i.loli.net/2021/04/20/7fnbVliHpwAS2u4.png" alt="quick_thought_vector.PNG"></p>
<p>对于一个给定的句子 $s$，判断 $s_{cand} (s_{cand}\in S_{cand})$ 是其上下文的概率为：</p>
<script type="math/tex; mode=display">p(s_{cand}|s,S_{cand})= \frac{exp[c(f(s),g(s_{cand}))]}{\sum_{s'\in S_{cand}}exp[c(f(s),g(s'))]}</script><p>上式中，$f$ 和 $g$ 分别为两个不同的 encoder（RNN 网络），$S_{cand}$ 包含一个正例和多个负例（即一个上下文句子和多个非上下文句子），$c$ 是打分函数，论文中将其简单定义为向量的内积，即 $c(u,v)=u^Tv$.</p>
<p>从上文求条件概率的等式中可以看出，该任务的目标函数是希望句子 $s$ 和其上下文句子的向量内积越大越好，反之则越小越好。Quick-Thoughts 训练完成之后，对于下游任务中的任一句子 $s$，通过拼接 $f(s)$ 和 $g(s)$ 计算得出的向量来得到 $s$ 对应的句向量。</p>
<h4 id="Doc2vec-模型"><a href="#Doc2vec-模型" class="headerlink" title="Doc2vec 模型"></a>Doc2vec 模型</h4><p>Doc2vec【<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1405.4053">论文</a>】是将 word2vec 的概念扩展到句子，段落或整个文章所形成的嵌入模型，借鉴 word2vec 中 CBOW 和 Skip-gram 的思想，其有两种添加句向量到模型中的方法。</p>
<p><strong>PV-DM(Distributed Memory version of Paragraph Vector)</strong><br>该方法与 CBOW 类似，通过上下文预测下一个词。但其在输入层额外添加了该上下文所对应的句向量，具体的网络结构如下所示：<br><img src="https://i.loli.net/2021/04/20/OAKUlLJrn592Ebe.png" alt="PV_DM.PNG"></p>
<p>训练结束后，对于现有的文档，可以直接通过网络训练后得到的句向量矩阵查询。而对于一篇新的文档，需要重新将其输入到网络中进行训练，但该过程会冻结模型里的词向量矩阵以及投影层到输出层的权重参数，只需要更新句向量，所以该过程会很快收敛。</p>
<p><strong>PV-DOBW( Distributed Bag of Words version of Paragraph Vector)</strong><br>该方法与 Skip-gram 类似，通过文档来预测文档内的词。训练时随机从预测文档内采样一些文本片段，然后再从这个片段中采样一个词作为 label，模型借助这一分类任务来训练句向量，具体的网络结构如下所示：<br><img src="https://i.loli.net/2021/04/20/o3Trieq8W2JuQxn.png" alt="PV-DBOW.PNG"></p>
<h3 id="有监督模型法"><a href="#有监督模型法" class="headerlink" title="有监督模型法"></a>有监督模型法</h3><h4 id="Deep-Averaging-Networks-DAN"><a href="#Deep-Averaging-Networks-DAN" class="headerlink" title="Deep Averaging Networks (DAN)"></a>Deep Averaging Networks (DAN)</h4><p>DAN 模型【<a target="_blank" rel="noopener" href="https://www.aclweb.org/anthology/P15-1162/">论文</a>】的实现方式比较简单和直观，首先将句子中的词向量求平均作为网络的输入嵌入表示，随后使该平均向量经过一个或多个前馈层，最后经过 softmax 层来完成相应的分类任务，具体的网络结构如下所示：</p>
<p><img src="https://i.loli.net/2021/04/20/zvXKNA5Bb4ykqhF.png" alt="DAN.PNG"></p>
<h4 id="Self-attentive-Sentence-Embedding"><a href="#Self-attentive-Sentence-Embedding" class="headerlink" title="Self-attentive Sentence Embedding"></a>Self-attentive Sentence Embedding</h4><p>该模型【<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1703.03130">论文</a>】通过引入 self-attention 机制来得到句向量，之前介绍的方法获得的词向量都是一维的向量表示，而该方法得到的句嵌入是二维的矩阵表示，矩阵的每一行表示对句子不同部分的<strong>注意</strong>，其网络结构如下所示：</p>
<p><img src="https://i.loli.net/2021/04/20/2jvumwdr6yXZNRf.png" alt="self-atteantion_sentence_embedding.PNG"></p>
<p>从上图(a)中可知，网络底层结构使用的是 Bi-LSTM，其隐层维度为 $u$，故而 $H\in R^{\,n\times 2u}$。图(b)是计算 self-attention 矩阵 $A$ 的过程</p>
<script type="math/tex; mode=display">A=softmax(W_{s2}tanh(W_{s1}H^T))</script><p>上式中，$W_{s1}\in R^{\,d_a\times 2u}$，$W_{s2}\in R^{\,r\times d_a}$，所以 $A\in R^{\,r\times n}$，最后用 self-attention 矩阵 A 与 隐向量矩阵 $H$ 相乘便可以得到最终的句子嵌入</p>
<script type="math/tex; mode=display">M = AH,\qquad M\in R^{\,r\times 2u}</script><h4 id="InferSent"><a href="#InferSent" class="headerlink" title="InferSent"></a>InferSent</h4><p>InferSent【<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1705.02364">论文</a>】是由 Facebook AI Research 提出的一种句嵌入方法，该方法通过训练 Natural Language Inference(NLI) 模型的方式来间接获取句向量，其通用的训练结构如下所示：<br><img src="https://i.loli.net/2021/04/20/enbToDVaXMvFdQL.png" alt="InferSent.PNG"><br>上图中 $u$ 和 $v$ 权值共享， $u$ 和 $v$ 也是我们最终需要获得的句向量。这个框架最底层的 <strong>sentence encoder</strong> 便是最终要获取的句向量提取器。</p>
<p>在训练过程中，<strong>premise</strong> 和 <strong>hypothesis</strong> 的句子嵌入以及它们按元素乘积和按元素求差的结果被级联在一起，拼接后的<strong>混合语义特征向量</strong>被送入多个全连接层，最后以3分类的softmax层（类别为entailment 蕴含，contradiction 矛盾，neutral中立）结束。</p>
<p>读到这里，细心的你应该会发现还有一个重要的问题没有解决，那就是 <strong>sentence encoder</strong> 的结构还没有确定。论文作者对比了7种不同的 <strong>sentence encoder</strong>，主要包括 RNN 与 CNN，其中 RNN 包括取 LSTM/GRU 最后一个隐状态，Bi-GRU/Bi-LSTM 的 mean/max pooling 以及 self-attention，其中取得最优效果的是 <strong>Bi-LSTM + max pooling</strong>，下面给出其具体结构：</p>
<p><img src="https://i.loli.net/2021/04/20/SMxuVcplE67aLRG.png" alt="InferSent_encoder.PNG"><br>由上图可知，<strong>sentence encoder</strong> 最终生成的句嵌入是通过将两个 LSTM 网络的输出向量进行级联，随后在每个维度取最大值的方式获得。</p>
<h4 id="Sentence-BERT"><a href="#Sentence-BERT" class="headerlink" title="Sentence-BERT"></a>Sentence-BERT</h4><p>Sentence-BERT 这篇【<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1908.10084">论文</a>】指出 Bert 在处理判断句子之间的相似度问题时所消耗的时间非常巨大（论文中给了一个小例子来阐述），这个问题的产生主要是受限于 Bert 的结构。但直接用 Bert 的输出作为句向量的效果比加权平均的 GloVe 还要差。基于上述问题，论文的作者提出了 Sentence-BERT 方法来获得效果较好的句向量。</p>
<p>SBERT 是在 <strong>siamese/triplet</strong> 网络结构上对 BERT 的微调，个人觉得其网络结构和 InferSent 的网络结构几乎没有太大的差异，文中 siamese 网络的具体结构如下图所示：</p>
<p><img src="https://i.loli.net/2021/04/21/ODGm1YaFugpXQsV.png" alt="SBERT.PNG"></p>
<p>从上图中可以看出，SBERT 和 InferSent 的主要不同点在于 <strong>sentence encoder</strong> 部分，SBERT 是使用 BERT 网络结构作为其<strong>sentence encoder</strong> 部分。下表给出其具体的池化策略和级联方式：</p>
<p><img src="https://i.loli.net/2021/04/21/wIKNWz78YQAC4uy.png" alt="SBERT_strategy.PNG"></p>
<h3 id="多任务学习模型法"><a href="#多任务学习模型法" class="headerlink" title="多任务学习模型法"></a>多任务学习模型法</h3><h4 id="Universal-Sentence-Encoder"><a href="#Universal-Sentence-Encoder" class="headerlink" title="Universal Sentence Encoder"></a>Universal Sentence Encoder</h4><p>Universal Sentence Encoder【<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.11175">论文</a>】是Google 于 2018 年初发布的通用句子编码器，其主要思想是在<strong>多个任务</strong>上对句子编码器在有监督和无监督的语料库（无监督训练数据包括问答(QA)、维基百科和网页新闻等，有监督训练数据为 SNLI）上进行训练，且编码器参数共享，从而学习出一个通用的句子编码器，这样通用句子编码器生成的句向量便可以应用到不同的 NLP 任务中，比如文本相似性判断，聚类和文本分类等。其具体网络结构如下图所示：</p>
<p><img src="https://i.loli.net/2021/04/21/mogq1PQC5DBONds.png" alt="Universal Sentence Encoder.PNG"></p>
<p>基于准确性和训练速度的折中，文中分别给出了 transformer 和 DAN 两种编码器结构。transformer 编码器有着更好的准确性，对于长句子来说内存开销和计算资源消耗都比较严重，导致计算时间显著增加，但对于短句子来说其计算速度还在可以接受的范围。DAN 编码器相比 transformer 准确性略有降低，但其计算速度快且资源消耗少。</p>
<p>Universal Sentence Encoder 的任务具体介绍以及各任务网络实现的细节可以参考<a href="#Reference">[6]</a>，这篇文章用可视化的方法详细介绍了Universal Sentence Encoder，通俗易懂，是很不错的学习资料。</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.01400">[1]. Concatenated Power Mean Word Embeddingsas Universal Cross-Lingual Sentence Representations</a><br><a target="_blank" rel="noopener" href="https://medium.com/data-from-the-trenches/how-deep-does-your-sentence-embedding-model-need-to-be-cdffa191cb53">[2]. How deep does your Sentence Embedding model need to be ?</a><br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/85739175">[3]. 关于句子表征的学习笔记</a><br><a target="_blank" rel="noopener" href="https://medium.datadriveninvestor.com/from-word-embeddings-to-sentence-embeddings-part-2-3-21a5b03592a1">[4]. From Word Embeddings to Sentence Embeddings — Part 2/3</a><br><a target="_blank" rel="noopener" href="https://www.analyticsvidhya.com/blog/2020/08/top-4-sentence-embedding-techniques-using-python/">[5]. Top 4 Sentence Embedding Techniques using Python!</a><br><a target="_blank" rel="noopener" href="https://amitness.com/2020/06/universal-sentence-encoder/">[6]. Universal Sentence Encoder Visually Explained</a></p>

    </div>

    
    
    

    <div>
      
        <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-fish"></i>感谢您的阅读-------------</div>
    
</div>
      
    </div>


    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/embedding/" rel="tag"><i class="fa fa-tag"></i> embedding</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/04/09/%E6%AF%8F%E6%97%A5%E8%AE%B0%E5%BD%95-%E8%BF%9B%E5%B1%95/" rel="prev" title="每日记录&进展">
                  <i class="fa fa-chevron-left"></i> 每日记录&进展
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>







<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2020 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Migo</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="//cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  







  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>





  <script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              const target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    const script = document.createElement('script');
    script.src = '//cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js';
    script.defer = true;
    document.head.appendChild(script);
  } else {
    MathJax.startup.document.state(0);
    MathJax.typesetClear();
    MathJax.texReset();
    MathJax.typeset();
  }
</script>



</body>
</html>
